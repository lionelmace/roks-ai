{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc5883ef-595f-4ce8-b716-824afc3a61c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python notebook to:\n",
    "# 1. Download model files from Hugging Face and store temporarily on node\n",
    "# 2. Upload model files to COS\n",
    "# 3. Utilites to clean up the temporarily stored model files\n",
    "#\n",
    "#\n",
    "# ***** Make sure the workbench is configured with at least 60GB storage.\n",
    "# ***** Make sure the current working directory has at least 40GB available disk space.\n",
    "# ***** Make sure to clean up the downloaded files on the the pod.\n",
    "#TBD Create a pipeline https://www.kubeflow.org/docs/components/pipelines/user-guides/core-functions/compile-a-pipeline/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d3a758f-7a69-48cb-85d1-14eff9fd0acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boto3==1.35.55 in /opt/app-root/lib64/python3.11/site-packages (1.35.55)\n",
      "Requirement already satisfied: botocore<1.36.0,>=1.35.55 in /opt/app-root/lib64/python3.11/site-packages (from boto3==1.35.55) (1.35.55)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/app-root/lib64/python3.11/site-packages (from boto3==1.35.55) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /opt/app-root/lib64/python3.11/site-packages (from boto3==1.35.55) (0.10.4)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/app-root/lib64/python3.11/site-packages (from botocore<1.36.0,>=1.35.55->boto3==1.35.55) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /opt/app-root/lib64/python3.11/site-packages (from botocore<1.36.0,>=1.35.55->boto3==1.35.55) (2.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/app-root/lib64/python3.11/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.36.0,>=1.35.55->boto3==1.35.55) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: botocore==1.35.55 in /opt/app-root/lib64/python3.11/site-packages (1.35.55)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/app-root/lib64/python3.11/site-packages (from botocore==1.35.55) (1.0.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/app-root/lib64/python3.11/site-packages (from botocore==1.35.55) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /opt/app-root/lib64/python3.11/site-packages (from botocore==1.35.55) (2.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/app-root/lib64/python3.11/site-packages (from python-dateutil<3.0.0,>=2.1->botocore==1.35.55) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: huggingface_hub in /opt/app-root/lib64/python3.11/site-packages (0.34.1)\n",
      "Requirement already satisfied: filelock in /opt/app-root/lib64/python3.11/site-packages (from huggingface_hub) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/app-root/lib64/python3.11/site-packages (from huggingface_hub) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/app-root/lib64/python3.11/site-packages (from huggingface_hub) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/app-root/lib64/python3.11/site-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in /opt/app-root/lib64/python3.11/site-packages (from huggingface_hub) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/app-root/lib64/python3.11/site-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/app-root/lib64/python3.11/site-packages (from huggingface_hub) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/app-root/lib64/python3.11/site-packages (from huggingface_hub) (1.1.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/app-root/lib64/python3.11/site-packages (from requests->huggingface_hub) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/app-root/lib64/python3.11/site-packages (from requests->huggingface_hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/app-root/lib64/python3.11/site-packages (from requests->huggingface_hub) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/app-root/lib64/python3.11/site-packages (from requests->huggingface_hub) (2025.1.31)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install boto3==1.35.55 \n",
    "!pip install botocore==1.35.55\n",
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "882e89eb-b7b3-4504-9120-72e282fde183",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import os\n",
    "import boto3\n",
    "import botocore\n",
    "import string\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6d34357-61ae-4bc1-9863-321b52e124e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURATION\n",
    "\n",
    "# Model to download from Hugging Face\n",
    "# Get from https://huggingface.co/models\n",
    "# E.g https://huggingface.co/ibm-granite/granite-3.3-8b-instruct\n",
    "#download_model_id='ibm-granite/granite-3.3-8b-instruct'\n",
    "\n",
    "download_model_id='meta-llama/Llama-3.2-3B-Instruct'\n",
    "#https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct/tree/main\n",
    "\n",
    "# COS bucket and credentials for upload\n",
    "access_key_id = \"0bceecea1e564a9aaf26cc67d27dd410\" \n",
    "secret_access_key = \"43f154c27aef623c39da5ffcdc857e517e5eabdcfcc522c5\" \n",
    "endpoint_url = \"https://s3.br-sao.cloud-object-storage.appdomain.cloud\"\n",
    "region_name = \"br-sao\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5129bb39-675e-4f46-a146-ea92490e6946",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "70a84cbe-7ac7-4cf3-a773-f418ede59b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of current directory: ['.cache', 'Untitled1.ipynb', 'test_model_download_pipeline.yaml', '.npm', 'test_model_all_download_pipeline.yaml', 'NB-pipeline-dev-helper.ipynb', 'NB-ai-agent-v2-loan-risk-for-rhoai.ipynb', '.ipynb_checkpoints', 'gs_model_pipeline.yaml', 'mcp3-ai-agent-bank-loan.ipynb', '.virtual_documents', '.config', 'writing_a_test_file.txt', 'lost+found', 'mcp3-wxai-ai-agent-bank-loan.ipynb', 'Untitled.ipynb', 'gs_hf_aws_pipeline.ipynb', 'file-write-pipeline.yaml', 'full_modeld_pipeline.yaml', '.ipython', '.local', 'NB-gen-rhoai-model-setup-pipeline.ipynb', '.locks', 'full_model_pipeline.yaml', 'my_output_model_path', '.jupyter', 'NB-get-and-upload-model.ipynb']\n",
      "Current directory:/opt/app-root/src\n",
      "Disk Usage for /opt/app-root/src:\n",
      "Total: 39.20 GB\n",
      "Used: 0.03 GB\n",
      "Free: 39.15 GB\n",
      "***********************************\n",
      "**** BEFORE PROCEEDING FURTHER ****\n",
      "Make sure the current working directory has at least 39GB available Free disk space.\n",
      "This space is needed to download the model repository/files from Hugging Face.\n",
      "***********************************\n"
     ]
    }
   ],
   "source": [
    "# Check current directory storage and contents\n",
    "# Get the current directory path\n",
    "current_directory = os.getcwd()  # Replace with the actual directory path\n",
    "#current_directory = '.'  # Current working directory\n",
    "\n",
    "current_directory_contents = os.listdir('.')\n",
    "# Print curent directoy contents\n",
    "print(f\"Contents of current directory: {current_directory_contents}\")\n",
    "\n",
    "# Get disk usage statistics\n",
    "total, used, free = shutil.disk_usage(current_directory)\n",
    "\n",
    "# Convert bytes to a more readable unit (e.g., GB)\n",
    "total_gb = total / (1024**3)\n",
    "used_gb = used / (1024**3)\n",
    "free_gb = free / (1024**3)\n",
    "\n",
    "# Print disk storage space\n",
    "print(f\"Current directory:{current_directory}\")\n",
    "print(f\"Disk Usage for {current_directory}:\")\n",
    "print(f\"Total: {total_gb:.2f} GB\")\n",
    "print(f\"Used: {used_gb:.2f} GB\")\n",
    "print(f\"Free: {free_gb:.2f} GB\")\n",
    "\n",
    "print(f\"***********************************\")\n",
    "print(f\"**** BEFORE PROCEEDING FURTHER ****\")\n",
    "print(f\"Make sure the current working directory has at least 39GB available Free disk space.\")\n",
    "print(f\"This space is needed to download the model repository/files from Hugging Face.\")\n",
    "print(f\"***********************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33f526b-dc43-41e6-910f-426c3aa0eaf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e3a4f1fe-6279-4843-8096-a618f2a1838e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm free disk storage space above before proceeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f2e73f-7c1c-496d-beab-1df1010e1361",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "747b6e50-1fe6-4e5e-890c-1f4b89b3ffdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91ed7747752c46778a4649e20b5890be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 16 files:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cabe1cf832af4c44b2128001cedc4742",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aacda7b1c0eb4db49cf7411f54ccebe6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/1.52k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b03bab2db6c4d64880e9fd935f7acc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "USE_POLICY.md:   0%|          | 0.00/6.02k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc5de6ccae404e84801d0f52730f6acb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/878 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a7c89b4498c4a5f9e40e8c7c4b63981",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "948f1d697d854535b2158ff71dc0c8c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fbf9fc0e8b64242b126fb0f871af558",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "params.json:   0%|          | 0.00/220 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9244c990d3247408371ba7f419523dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "original/tokenizer.model:   0%|          | 0.00/2.18M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efbd62be1811477daa30b931316bab4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5a5564cc6b340e2bc6efc3c6f7473dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daa99829f658478980d5b61087994c80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e753bc575614eacaf830af4fec7c4a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f3ebc763e1349dba7f736a42c1b56c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "original/consolidated.00.pth:   0%|          | 0.00/6.43G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository downloaded to: ./models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95\n"
     ]
    }
   ],
   "source": [
    "# Download the model from Hugging Face \n",
    "# This step will take a few minutes.\n",
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "repo_path = snapshot_download(\n",
    "    #use_auth_token=\"hf_dXXXXXXXXXXXXXXXXX\", # hf auth required for some models\n",
    "    repo_id=download_model_id,\n",
    "    repo_type=\"model\",\n",
    "    cache_dir=\".\"\n",
    ")\n",
    "print(f\"Repository downloaded to: {repo_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2ddd9571-8640-47a4-8cd8-9285f5d21d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository downloaded to local_models_directory: ./models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95\n"
     ]
    }
   ],
   "source": [
    "local_models_directory=repo_path\n",
    "print(f\"Repository downloaded to local_models_directory: {local_models_directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d316d874-f578-4d63-af47-7126ef30d246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of model directory:\n",
      "['README.md', 'model-00002-of-00002.safetensors', 'tokenizer.json', 'generation_config.json', 'model.safetensors.index.json', 'config.json', 'USE_POLICY.md', '.gitattributes', 'original', 'tokenizer_config.json', 'special_tokens_map.json', 'model-00001-of-00002.safetensors', 'LICENSE.txt']\n"
     ]
    }
   ],
   "source": [
    "print(\"Contents of model directory:\")\n",
    "local_models_directory_contents = os.listdir(local_models_directory)\n",
    "print(local_models_directory_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd621cf-2e60-4f9f-a107-b8c14af47c38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ebce8c0-5093-41c0-a086-569c75fa6163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload model files to IBM Cloud COS instance bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "46c13236-5c37-4ae9-96dc-5876beb4d4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an S3 resource object with credentials\n",
    "s3_resource = boto3.resource(\n",
    "    \"s3\",\n",
    "    aws_access_key_id=access_key_id,\n",
    "    aws_secret_access_key=secret_access_key,\n",
    "    region_name=region_name,\n",
    "    endpoint_url=endpoint_url\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "adc62131-bce9-45a1-a366-d1765de50d17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created bucket for the model files: llama-3.2-3b-instruct-wglo4\n"
     ]
    }
   ],
   "source": [
    "# Create the bucket\n",
    "# Model bucket name is generated from the model name. No configuration needed\n",
    "# e.g., granite-3.3-8b-instruct-<suffix> , 'granite-3.3-8b-instruct-HJ8mKd'\n",
    "model_files_bucket_name = (download_model_id.split('/')[1]+'-'+''.join(random.choices(string.ascii_letters + string.digits, k=5))).lower()\n",
    "\n",
    "model_files_bucket = s3_resource.create_bucket(\n",
    "    ACL='private',\n",
    "    Bucket=model_files_bucket_name,\n",
    "    # By default bucket of type standard is created.\n",
    "    # To create bucket of type smart tier, uncomment code below\n",
    "    # Refer: https://cloud.ibm.com/docs/cloud-object-storage?topic=cloud-object-storage-classes#classes\n",
    "    #CreateBucketConfiguration={\n",
    "    #    'LocationConstraint': region_name+'-smart' \n",
    "    #}\n",
    ")\n",
    "\n",
    "#print(model_files_bucket)\n",
    "print(f\"Created bucket for the model files: {model_files_bucket_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "93443852-886f-4d1d-b277-4e364631ffc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the model files to the bucket\n",
    "# function definitions\n",
    "def list_objects(bucket, prefix):\n",
    "    filter = bucket.objects.filter(Prefix=prefix)\n",
    "    for obj in filter.all():\n",
    "        print(obj.key)\n",
    "    #To list all files use: list_objects(model_files_bucket'')\n",
    "    #To list all files with a prefix eg model-files use: list_objects(model_files_bucket,'model-files')\n",
    "    #To change to a different bucket\n",
    "    #bucket_to_list=s3_resource.Bucket(\"granite-3.3-8b-instruct-v0711\")\n",
    "\n",
    "        \n",
    "def upload_directory_to_s3(local_directory, s3_prefix):\n",
    "    num_files = 0\n",
    "    for root, dirs, files in os.walk(local_directory):\n",
    "        for filename in files:\n",
    "            file_path = os.path.join(root, filename)\n",
    "            relative_path = os.path.relpath(file_path, local_directory)\n",
    "            s3_key = os.path.join(s3_prefix, relative_path)\n",
    "            print(f\"{file_path} -> {s3_key}\")\n",
    "            model_files_bucket.upload_file(file_path, s3_key)\n",
    "            num_files += 1\n",
    "    return num_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "367e74d2-58bf-40de-8d45-3387a8bf09d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Starting to upload model files to bucket: llama-3.2-3b-instruct-wglo4 ***\n",
      "This step will take a few minutes. Progress updates below as each file gets uploaded.\n",
      "./models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/README.md -> model-files/README.md\n",
      "./models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/model-00002-of-00002.safetensors -> model-files/model-00002-of-00002.safetensors\n",
      "./models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/tokenizer.json -> model-files/tokenizer.json\n",
      "./models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/generation_config.json -> model-files/generation_config.json\n",
      "./models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/model.safetensors.index.json -> model-files/model.safetensors.index.json\n",
      "./models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json -> model-files/config.json\n",
      "./models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/USE_POLICY.md -> model-files/USE_POLICY.md\n",
      "./models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/.gitattributes -> model-files/.gitattributes\n",
      "./models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/tokenizer_config.json -> model-files/tokenizer_config.json\n",
      "./models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/special_tokens_map.json -> model-files/special_tokens_map.json\n",
      "./models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/model-00001-of-00002.safetensors -> model-files/model-00001-of-00002.safetensors\n",
      "./models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/LICENSE.txt -> model-files/LICENSE.txt\n",
      "./models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/original/orig_params.json -> model-files/original/orig_params.json\n",
      "./models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/original/tokenizer.model -> model-files/original/tokenizer.model\n",
      "./models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/original/consolidated.00.pth -> model-files/original/consolidated.00.pth\n",
      "./models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/original/params.json -> model-files/original/params.json\n",
      "*** Completed uploading model files to bucket: llama-3.2-3b-instruct-wglo4 ***\n",
      " \n",
      "Saved 16 model files to the bucket \"llama-3.2-3b-instruct-wglo4\"\n",
      "Each file has path prefix \"model-files\". This path will be required during model deployment configuration\n",
      " \n",
      "Here is the list of files uploaded to IBM Cloud COS bucket \"llama-3.2-3b-instruct-wglo4\"\n",
      "model-files/.gitattributes\n",
      "model-files/LICENSE.txt\n",
      "model-files/README.md\n",
      "model-files/USE_POLICY.md\n",
      "model-files/config.json\n",
      "model-files/generation_config.json\n",
      "model-files/model-00001-of-00002.safetensors\n",
      "model-files/model-00002-of-00002.safetensors\n",
      "model-files/model.safetensors.index.json\n",
      "model-files/original/consolidated.00.pth\n",
      "model-files/original/orig_params.json\n",
      "model-files/original/params.json\n",
      "model-files/original/tokenizer.model\n",
      "model-files/special_tokens_map.json\n",
      "model-files/tokenizer.json\n",
      "model-files/tokenizer_config.json\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Upload the model files to the bucket\n",
    "# This steps can take a few minutes.\n",
    "\n",
    "if not os.path.isdir(local_models_directory):\n",
    "    raise ValueError(f\"The directory '{local_models_directory}' does not exist.\"\n",
    "                     \"Check if model was correctly downloaded in previous steps.\")\n",
    "else:\n",
    "\n",
    "    print(f\"*** Starting to upload model files to bucket: {model_files_bucket_name} ***\")\n",
    "    print(f\"This step will take a few minutes. Progress updates below as each file gets uploaded.\")\n",
    "    model_files_prefix=\"model-files\"\n",
    "    num_files = upload_directory_to_s3(local_models_directory, model_files_prefix)\n",
    "    # NOTE: \"model-files\" is a prefix added to each file that is uploaded. A path is required for configuration in RHOAI.\n",
    "    print(f\"*** Completed uploading model files to bucket: {model_files_bucket_name} ***\")\n",
    "    if num_files == 0:\n",
    "        raise ValueError(\"No files uploaded. \")\n",
    "    else:\n",
    "        print(f\" \")\n",
    "        print(f\"Saved {num_files} model files to the bucket \\\"{model_files_bucket_name}\\\"\")\n",
    "        print(f\"Each file has path prefix \\\"{model_files_prefix}\\\". This path will be required during model deployment configuration\")\n",
    "        print(f\" \")\n",
    "        print(f\"Here is the list of files uploaded to IBM Cloud COS bucket \\\"{model_files_bucket_name}\\\"\")\n",
    "        print(list_objects(model_files_bucket, 'model-files'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd016c6-a368-4987-94e3-a1531b5882aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "11ec3ec1-3cd8-4463-b00f-51f7cddfeb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **** CLEAN UP ****\n",
    "# **** PROCEED WITH CAUTION ****\n",
    "# **** CLEAN UP ****\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "68b1b1f8-48a5-435c-8dab-5c3d149ed494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of current directory: ['.cache', 'Untitled1.ipynb', 'test_model_download_pipeline.yaml', '.npm', 'test_model_all_download_pipeline.yaml', 'NB-pipeline-dev-helper.ipynb', 'NB-ai-agent-v2-loan-risk-for-rhoai.ipynb', '.ipynb_checkpoints', 'gs_model_pipeline.yaml', 'mcp3-ai-agent-bank-loan.ipynb', 'models--meta-llama--Llama-3.2-3B-Instruct', '.virtual_documents', '.config', 'writing_a_test_file.txt', 'lost+found', 'mcp3-wxai-ai-agent-bank-loan.ipynb', 'Untitled.ipynb', 'gs_hf_aws_pipeline.ipynb', 'file-write-pipeline.yaml', 'full_modeld_pipeline.yaml', '.ipython', '.local', 'NB-gen-rhoai-model-setup-pipeline.ipynb', '.locks', 'full_model_pipeline.yaml', 'my_output_model_path', '.jupyter', 'NB-get-and-upload-model.ipynb']\n"
     ]
    }
   ],
   "source": [
    "# **** PROCEED WITH CAUTION ****\n",
    "\n",
    "# Once uploaded to IBM COS bucket, use the utilities below to clean up and remove the model directory and its files\n",
    "# Removing files will free up storage space on this workbench\n",
    "# First, list the current directories\n",
    "current_directory_contents = os.listdir('.')\n",
    "print(f\"Contents of current directory: {current_directory_contents}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6ecba51-1bef-4cb8-bf8b-e8f09ff1b80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Model directory to remove: /opt/app-root/src/models--ibm-granite--granite-3.3-8b-instruct\n",
      "*** Contents of model directory to remove: ['snapshots', 'blobs', 'refs']\n",
      "*** Cache directory to remove: /opt/app-root/src/.cache/huggingface\n",
      "*** Contents of cache directory to remove: ['xet']\n"
     ]
    }
   ],
   "source": [
    "# Set the local model directory and cache for removal\n",
    "directory_to_remove=local_models_directory.split(os.sep)[1] #or set this to any other directory 'models--ibm-granite--granite-3.3-8b-instruct'\n",
    "#directory_to_remove=\"models--ibm-granite--granite-3.3-8b-instruct\"\n",
    "#directory_to_remove=\"models--meta-llama--Llama-3.2-3B-Instruct\"\n",
    "cache_directory_to_remove=\".cache/huggingface\"\n",
    "\n",
    "path_to_directory_to_remove = os.path.join(current_directory, directory_to_remove)\n",
    "path_to_cache_directory_to_remove = os.path.join(current_directory, cache_directory_to_remove)\n",
    "\n",
    "if os.path.exists(path_to_directory_to_remove) and os.path.isdir(path_to_directory_to_remove):\n",
    "    print(f\"*** Model directory to remove: {path_to_directory_to_remove}\")\n",
    "    directory_to_remove_contents = os.listdir(path_to_directory_to_remove)\n",
    "    print(f\"*** Contents of model directory to remove: {directory_to_remove_contents}\")\n",
    "else:\n",
    "    print(\"Incorrect directory name or directory does not exist.\")\n",
    "if os.path.exists(cache_directory_to_remove):\n",
    "    print(f\"*** Cache directory to remove: {path_to_cache_directory_to_remove}\")\n",
    "    cache_directory_to_remove_contents = os.listdir('.cache/huggingface')\n",
    "    print(f\"*** Contents of cache directory to remove: {cache_directory_to_remove_contents}\")\n",
    "else:\n",
    "    print(\"Cache directory for does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449f2087-73ed-48e2-a44f-cbb7851c10fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a5e153-f652-469b-ba22-ca6415dd99cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46c32908-5bc6-4ded-8101-b07e1142dbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: \n",
    "# Running this cell will remove the local model directory and its content files (identified above).\n",
    "# The files are not needed and can be removed if they were uplaoded to IBM Cloud COS bucket.\n",
    "# Removing files will free up storage space on this workbench\n",
    "\n",
    "if os.path.exists(path_to_directory_to_remove) and os.path.isdir(path_to_directory_to_remove):\n",
    "    shutil.rmtree(path_to_directory_to_remove)\n",
    "if os.path.exists(path_to_cache_directory_to_remove): \n",
    "    shutil.rmtree(path_to_cache_directory_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "391394dc-fa43-45e0-b092-21374ce8ddf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of current directory: ['.cache', 'Untitled1.ipynb', 'test_model_download_pipeline.yaml', '.npm', 'test_model_all_download_pipeline.yaml', 'NB-pipeline-dev-helper.ipynb', 'NB-ai-agent-v2-loan-risk-for-rhoai.ipynb', '.ipynb_checkpoints', 'gs_model_pipeline.yaml', 'mcp3-ai-agent-bank-loan.ipynb', 'models--meta-llama--Llama-3.2-3B-Instruct', '.virtual_documents', '.config', 'writing_a_test_file.txt', 'lost+found', 'mcp3-wxai-ai-agent-bank-loan.ipynb', 'Untitled.ipynb', 'gs_hf_aws_pipeline.ipynb', 'file-write-pipeline.yaml', 'full_modeld_pipeline.yaml', '.ipython', '.local', 'NB-gen-rhoai-model-setup-pipeline.ipynb', '.locks', 'full_model_pipeline.yaml', 'my_output_model_path', '.jupyter', 'NB-get-and-upload-model.ipynb']\n"
     ]
    }
   ],
   "source": [
    "# Confirm the model directory is removed. It should not show up in teh list below.\n",
    "current_directory_contents = os.listdir('.')\n",
    "print(f\"Contents of current directory: {current_directory_contents}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "628ff906-2126-4eed-ba87-fe0d990af09c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory:/opt/app-root/src\n",
      "Disk Usage for /opt/app-root/src:\n",
      "Total: 39.20 GB\n",
      "Used: 0.03 GB\n",
      "Free: 39.15 GB\n"
     ]
    }
   ],
   "source": [
    "# Get disk usage statistics again to confirm free space.\n",
    "total, used, free = shutil.disk_usage(current_directory)\n",
    "\n",
    "# Convert bytes to a more readable unit (e.g., GB)\n",
    "total_gb = total / (1024**3)\n",
    "used_gb = used / (1024**3)\n",
    "free_gb = free / (1024**3)\n",
    "\n",
    "current_directory_contents = os.listdir('.')\n",
    "# Print the results\n",
    "print(f\"Current directory:{current_directory}\")\n",
    "print(f\"Disk Usage for {current_directory}:\")\n",
    "print(f\"Total: {total_gb:.2f} GB\")\n",
    "print(f\"Used: {used_gb:.2f} GB\")\n",
    "print(f\"Free: {free_gb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cfc94b-fe28-4a25-aeed-7954cfb6c36f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
